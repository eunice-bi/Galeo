{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import xlrd\n",
    "import numpy as np\n",
    "from numpy import nan\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.plotly as py\n",
    "import plotly\n",
    "plotly.tools.set_credentials_file(username='eadrien', api_key='KnEjzGXF14YNufp5E9xs')\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "import calendar\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "def remove_duplicates(values):\n",
    "    output = []\n",
    "    output_index = []\n",
    "    seen = set()\n",
    "    i = 0\n",
    "    for value in values:\n",
    "        # If value has not been encountered yet,\n",
    "        # ... add it to both list and set.\n",
    "        if value not in seen:\n",
    "            output.append(value)\n",
    "            output_index.append(i)\n",
    "            seen.add(value)\n",
    "        i += 1\n",
    "    return output_index\n",
    "\n",
    "\n",
    "# In[5]:\n",
    "\n",
    "\n",
    "def date_format(data_frame):\n",
    "    data_transpose = data_frame.transpose()\n",
    "    data_transpose = data_transpose.iloc[:,1:]\n",
    "    df_date = pd.DataFrame(data_transpose.iloc[0,:])\n",
    "    df_time = pd.DataFrame(data_transpose.iloc[1,:])\n",
    "    for i in range(1,len(df_date)):\n",
    "        df_date.iloc[i][0] = df_date.iloc[i][0].replace(hour=int(str(df_time.iloc[i])[5:7]), minute=int(str(df_time.iloc[i])[8:10]), second=int(str(df_time.iloc[i])[11:13]))\n",
    "    df_date_row = df_date.transpose()\n",
    "    data_transpose = data_transpose.drop([0],axis=0)\n",
    "    data_transpose = data_transpose.drop([1],axis=0)\n",
    "    data_transpose = data_transpose.append(df_date_row, ignore_index=False)\n",
    "    data_transpose = data_transpose.sort_index()\n",
    "    number_columns_df = len(data_transpose.columns)\n",
    "    data_transpose.columns = range(number_columns_df)\n",
    "    data_transpose = data_transpose.reset_index()\n",
    "    data_transpose = data_transpose.drop(\"index\",axis=1)\n",
    "    return data_transpose\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n",
    "def remove_duplicates_in_df(df):\n",
    "    list_names_columns = df.iloc[:,0]\n",
    "    list_index_names_columns_not_duplicated = remove_duplicates(list_names_columns)\n",
    "    df = df.loc[list_index_names_columns_not_duplicated]\n",
    "    return df\n",
    "\n",
    "\n",
    "# In[7]:\n",
    "\n",
    "\n",
    "def no_na_preparation(df):\n",
    "    df_nona = pd.DataFrame(df)\n",
    "    df_nona = df_nona.transpose()\n",
    "    df_nona_t = df_nona.iloc[1:,1:].dropna(axis=1,how=\"all\")\n",
    "    #fulfill empty values by copying the previous full value in the column\n",
    "    df_nona_t = df_nona_t.fillna( method='backfill', axis=0)\n",
    "    #fuflill empty values by copying the next full value in the column\n",
    "    df_nona_t = df_nona_t.fillna( method='ffill', axis=0)\n",
    "    df_nona_t = df_nona_t.astype(float)\n",
    "    return df_nona_t\n",
    "\n",
    "\n",
    "# In[8]:\n",
    "\n",
    "\n",
    "def get_new_column_names(df, df2,df_names):\n",
    "    list_noms_colonnes = []\n",
    "    list_noms_colonnes_avant = df_names\n",
    "    for i in range(len(df2.columns.values)):\n",
    "        list_noms_colonnes.append(list_noms_colonnes_avant[df2.columns.values[i]])\n",
    "    return list_noms_colonnes\n",
    "\n",
    "\n",
    "# In[9]:\n",
    "\n",
    "\n",
    "def format_df(df,df2,data_unités,df_names):\n",
    "    list_noms_colonnes = get_new_column_names(df,df2,df_names)\n",
    "    list_adress_units_files = data_unités[\"Adresse\"]\n",
    "    list_text_units_files = data_unités[\"Texte\"]\n",
    "    list_units = data_unités[\"Unité\"]\n",
    "    list_indexes = []\n",
    "    for i in range(len(list_noms_colonnes )):\n",
    "        bool = 0\n",
    "        for j in range(len(list_adress_units_files)):\n",
    "            if(bool == 1):\n",
    "                break\n",
    "            elif(list_noms_colonnes[i] == list_adress_units_files[j]):\n",
    "                list_indexes.append(j)\n",
    "                bool = 1\n",
    "        if(bool == 0):\n",
    "            list_indexes.append(-1)\n",
    "    new_list_texte = []\n",
    "    for i in range(len(list_indexes)):\n",
    "        new_list_texte.append(data_unités[\"Texte\"].iloc[list_indexes[i]])\n",
    "    #ew_list_texte.append('Date')\n",
    "    new_list_units = []\n",
    "    for i in range(len(list_indexes)):\n",
    "        new_list_units.append(data_unités[\"Unité\"].iloc[list_indexes[i]])\n",
    "    #new_list_units.append('Date')\n",
    "\n",
    "    #ist_noms_colonnes.append('Date')\n",
    "    df2.loc['Adress'] = list_noms_colonnes\n",
    "    df2.loc[\"Texte\"] = new_list_texte\n",
    "    df2.loc[\"Unité\"] = new_list_units\n",
    "    number_columns_df = len(df2.columns)\n",
    "    df2.columns = range(number_columns_df)\n",
    "    return df2\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "def check_columns_with_unique_values(df):\n",
    "    list_indexes = []\n",
    "    for i in range(len(df.columns)):\n",
    "        #array = df[names_of_columns[i]].unique()\n",
    "        array = pd.unique(df.iloc[:,i].values)\n",
    "        if len(array) == 1 :\n",
    "            list_indexes.append(i)\n",
    "    return list_indexes\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "def preparation_data(df,data_unités):\n",
    "    data = date_format(df)\n",
    "    data = remove_duplicates_in_df(data)\n",
    "    data_model = data.copy()\n",
    "    data_not_duplicated = no_na_preparation(data)\n",
    "    data_final = format_df(data,data_not_duplicated ,data_unités,data.iloc[1:,0])\n",
    "    data_final_just_data = data_final.iloc[:-3,:-1]\n",
    "    list_indexes_to_delete = check_columns_with_unique_values(data_final_just_data)\n",
    "    data_final_just_data_no_duplicated = data_final_just_data.copy()\n",
    "    data_final_just_data_no_duplicated.drop(columns = data_final_just_data.columns[list_indexes_to_delete],axis=1,inplace=True)\n",
    "    #list_noms_colonnes = get_new_column_names(data_final,data_final_just_data_no_duplicated,data_final.loc['Adress'])\n",
    "    #list_noms_colonnes.append('Date')\n",
    "    copy_data_final_just_data_no_duplicated = data_final_just_data_no_duplicated.copy()\n",
    "    data_final = format_df(data_final,copy_data_final_just_data_no_duplicated, data_unités,data_final.loc['Adress'])\n",
    "    list_date = data_model.iloc[0,:]\n",
    "    data_final_just_data_no_duplicated['Date'] = list_date\n",
    "    data_final['Date'] =list_date\n",
    "    return data_final, data_final_just_data_no_duplicated\n",
    "\n",
    "\n",
    "# In[41]:\n",
    "\n",
    "\n",
    "def Excel_for_Power_BI(df,df_model):\n",
    "    list_new_names_columns = []\n",
    "    df_copy = df.copy()\n",
    "    for i in range(len(df_copy.columns)):\n",
    "        list_new_names_columns.append(str(df_model.loc['Adress',i])+' en '+ str(df_model.loc['Unité',i]))\n",
    "    df_copy.columns = list_new_names_columns\n",
    "    save_df_in_excel(filename[:-6]+'_PowerBi.xlsx', df)\n",
    "    return 'Excel for Power BI generated'\n",
    "\n",
    "\n",
    "def Excel_to_look(df,df_model):\n",
    "    return 'test coucou'\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "def save_df_in_excel(filename, df):\n",
    "    writer = pd.ExcelWriter(filename)\n",
    "    df.to_excel(writer,\"Sheet\") \n",
    "    writer.save()\n",
    "\n",
    "\n",
    "# In[14]:\n",
    "\n",
    "\n",
    "data = pd.read_excel(\"13-06_13-07.xlsx\",header=None)\n",
    "data2 = pd.read_excel(\"13-07_27-09.xlsx\",header=None)\n",
    "data_unités = pd.read_excel('UnitésV6.xlsx')\n",
    "\n",
    "\n",
    "# In[19]:\n",
    "\n",
    "\n",
    "data_1, data_1_just_data = preparation_data(data,data_unités)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Excel_for_Power_BI(df,df_model,filename):\n",
    "    list_new_names_columns = []\n",
    "    df_copy = df.copy()\n",
    "    for i in df_model.columns:\n",
    "        list_new_names_columns.append(str(df_model.loc['Texte',i]) + str(df_model.loc['Adress',i])+' en '+ str(df_model.loc['Unité',i]))\n",
    "    list_new_names_columns[-1] = 'Date'\n",
    "    temp = list_new_names_columns[0]\n",
    "    list_new_names_columns[0] = 'Date'\n",
    "    list_new_names_columns[-1]  = temp\n",
    "    df_copy = df_copy[list_new_names_columns]\n",
    "    #df_copy.columns = list_new_names_columns\n",
    "    save_df_in_excel(filename+'_PowerBi.xlsx',df_copy)\n",
    "    return 'Excel for Power BI generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Excel_to_look(df,df_model,filename):\n",
    "    df_min = df_model.iloc[:-3,:-1].min()\n",
    "    df_median = df_model.iloc[:-3,:-1].median()\n",
    "    df_max = df_model.iloc[:-3,:-1].max()\n",
    "    df_adresse = df_model.loc[\"Adress\"][:-1]\n",
    "    df_adresse = pd.DataFrame(data=df_adresse)\n",
    "    #df_adresse.index = df_median.index.values\n",
    "    \n",
    "    min_adresse = pd.concat([df_adresse,df_min],axis=1,ignore_index=True)\n",
    "    #min_median = pd.DataFrame(min_median)\n",
    "    min_median = pd.concat([min_adresse,df_median],axis=1,ignore_index=True)\n",
    "    #min_median = pd.DataFrame(min_median)\n",
    "    min_median_df_inter = pd.concat([min_median, df_max],axis=1,ignore_index=True)\n",
    "    df_unité = df_model.loc[\"Unité\"][:-1]\n",
    "    min_median_df = pd.concat([min_median_df_inter, df_unité],axis=1,ignore_index=True)\n",
    "    min_median_df = pd.DataFrame(min_median_df)\n",
    "    min_median_df.columns = ['adresse','min','median','max','unité']\n",
    "    min_median_df.index = df_model.loc['Texte'][:-1]\n",
    "    save_df_in_excel(filename+'_Observations.xlsx',min_median_df)\n",
    "    return 'Excel for Observations generated'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excel for Power BI generated'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Excel_for_Power_BI(data_1_just_data,data_1,'test_13_07')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Excel for Observations generated'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Excel_to_look(data_1_just_data,data_1,'test_13_07')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_in_excel('test_r2.xlsx', data_1_just_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rpy2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-344-e79375ef17ff>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobjects\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrobjects\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mrobjects\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msource\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test_r.R'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rpy2'"
     ]
    }
   ],
   "source": [
    "import rpy2.robjects as robjects\n",
    "robjects.r.source('test_r.R')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\paule\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: FutureWarning:\n",
      "\n",
      "from_csv is deprecated. Please use read_csv(...) instead. Note that some of the default arguments are different, so please refer to the documentation for from_csv when changing your function calls\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File b'myfile.csv' does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-339-155be8cb8903>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_just_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'myfile.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mfrom_csv\u001b[1;34m(cls, path, header, sep, index_col, parse_dates, encoding, tupleize_cols, infer_datetime_format)\u001b[0m\n\u001b[0;32m   1577\u001b[0m                           \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1578\u001b[0m                           \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtupleize_cols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtupleize_cols\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1579\u001b[1;33m                           infer_datetime_format=infer_datetime_format)\n\u001b[0m\u001b[0;32m   1580\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1581\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mto_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'block'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 440\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    442\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'has_index_names'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    786\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 787\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    788\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1012\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'c'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1014\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1015\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1016\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'python'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'usecols'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1708\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1709\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: File b'myfile.csv' does not exist"
     ]
    }
   ],
   "source": [
    "df_just_data = pd.DataFrame.from_csv('myfile.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
