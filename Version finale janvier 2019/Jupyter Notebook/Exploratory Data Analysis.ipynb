{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import copy\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the correlation between a feature and the target\n",
    "def get_correlation_target(df,index_column,target):\n",
    "    return stats.pearsonr(df.iloc[:,index_column],target)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the list of adresses in regards of a column name\n",
    "def get_list_adress_from_columns(df,list_columns):\n",
    "    list_adress = df.iloc[-2,:]\n",
    "    list_adress.index = range(len(list_adress))\n",
    "    for i, text in enumerate(list_adress):\n",
    "        if text == 0 or text == ' ' or pd.isna(text)==True :\n",
    "            list_adress[i] = list_columns[i]\n",
    "    list_adress.index = list_columns \n",
    "    return list_adress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the data frame by removing Date, getting the list of columns and adresses \n",
    "def prepare_df_to_get_correlations(df,bool_validation):\n",
    "    df = df.drop(columns='Date')\n",
    "    df.columns = range(len(df.columns))\n",
    "    list_columns = df.iloc[-3,:]\n",
    "    #list_columns.index = range(len(list_columns))\n",
    "    #list_columns = list_columns.reset_index(drop=True,inplace=False)\n",
    "    size = len(list_columns)\n",
    "    list_columns.update(pd.Series(['WEEKDAYS', 'MONTHS','QUARTERS','Energie'], index=[size-4, size-3,size-2,size-1]))\n",
    "    if(bool_validation == 0):\n",
    "        list_adress = get_list_adress_from_columns(df,list_columns)\n",
    "    df.columns = range(len(df.columns))\n",
    "    df = df.iloc[:-3,:]\n",
    "    df_energie = df.iloc[:,-1]\n",
    "    df_energie_kw = df_energie/0.25\n",
    "    df.iloc[:,-1] = df_energie_kw\n",
    "    df.Energie = df_energie_kw \n",
    "    if(bool_validation == 0):\n",
    "        return df, list_adress, list_columns\n",
    "    else :\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of the data frame\n",
    "def normalization(df,bool_validation):\n",
    "    if(bool_validation) == 1:\n",
    "        remove_na_bug_training_validation_sets(df)\n",
    "    scaler.fit(df)\n",
    "    return pd.DataFrame(scaler.transform(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the time step of the data frame\n",
    "def df_changed_time_step(df,step_in_15minutes,bool_validation):\n",
    "    df_changed_time_step = pd.DataFrame(df.iloc[0:step_in_15minutes].median(axis=0)).transpose()\n",
    "    for i in range(1,int(len(df.index)/step_in_15minutes)):\n",
    "        df_changed_time_step.loc[i] = (df.iloc[step_in_15minutes*i:step_in_15minutes*(i+1)].median(axis=0)).transpose()\n",
    "    df_norm_changed_time_step = normalization(df_changed_time_step,bool_validation)\n",
    "    return df_norm_changed_time_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We build a list of Energy data by taking the median value of the given step\n",
    "def add_Energie_median(df,step_in_15minutes):\n",
    "    list_energie = [df.iloc[:,-1][1:step_in_15minutes].median()]\n",
    "    for i in range(1,int(len(df.index)/step_in_15minutes)):\n",
    "        list_energie.append(df.iloc[:,-1][step_in_15minutes*i:step_in_15minutes*(i+1)].median())\n",
    "    return list_energie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First step to find correlations between the features and the target\n",
    "def build_correlation_matrix(df,target,correlation_level):\n",
    "    list_correlations = [get_correlation_target(df,i,target) for i in range(len(df.columns))]\n",
    "    df_correlations = pd.DataFrame([list_correlations],columns=list_columns[:-1]).transpose()\n",
    "    df_correlations.columns = ['Corrélation avec Energie totale']\n",
    "    df_correlations[\"Texte\"] = list_adress[:-1]\n",
    "    df_correlations_correlation_level = df_correlations[abs(df_correlations['Corrélation avec Energie totale'])>correlation_level]\n",
    "    return df_correlations_correlation_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second step to keep the most correlated features\n",
    "def build_correlation_matrix_with_correlation_level(df_corr,target,correlation_level):\n",
    "    list_correlations = [get_correlation_target(df_corr,i,target) for i in range(len(df_corr.columns))]\n",
    "    df_correlations = pd.DataFrame([list_correlations],columns=df_corr.columns).transpose()\n",
    "    df_correlations.columns = ['Corrélation avec Energie totale']\n",
    "    df_correlations[\"Texte\"] = list_adress[:-1]\n",
    "    df_correlations_correlation_level = df_correlations[abs(df_correlations['Corrélation avec Energie totale'])>correlation_level]\n",
    "    return df_correlations_correlation_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function launch correlations matrix building\n",
    "def launch_correlations(df,target,name,correlation_level):\n",
    "    #list_columns = list_columns.tolist()\n",
    "    df_correlations_correlation_level = build_correlation_matrix(df,target,correlation_level)\n",
    "    name_columns_correlations_correlation_level = df_correlations_correlation_level.index.values.tolist() \n",
    "    list_columns_str = str(list_columns)\n",
    "    df_columns = pd.DataFrame(list_columns)\n",
    "    #list_columns = list_columns.reset_index(drop=True,inplace=False)\n",
    "    list_index = [df_columns.index[df_columns[\"Adress\"] == val][0] for val in name_columns_correlations_correlation_level]\n",
    "    pickle.dump(df , open( name+\".p\", \"wb\" ) )\n",
    "    pickle.dump(df.Energie , open( \"target\"+name+\".p\", \"wb\" ) )\n",
    "    name_corr = name + \"_corr.p\"\n",
    "    name_index =  \"list_index_\"+name+\".p\"\n",
    "    pickle.dump(df_correlations_correlation_level, open(name_corr, \"wb\" ) )\n",
    "    pickle.dump(list_index, open( name_index, \"wb\" ) )\n",
    "    return df_correlations_correlation_level, list_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the duplicated elements in a list\n",
    "def list_without_duplicates(myList):\n",
    "    y = list(set(myList))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide by 2 the number of correlated features by organizing by two the features based on their correlation \n",
    "#and keeping the most correlated feature to the target\n",
    "def remove_too_much_correlations(df_norm,df_corr,list_text,list_index_corr,num,var):\n",
    "    list_text_corr = list_text[list_index_corr]\n",
    "    df_norm_just_corr = df_norm.iloc[:,list_index_corr]\n",
    "    df_norm_just_corr.columns = range(len(df_norm_just_corr.columns))\n",
    "    df_norm_just_corr.index = range(len(df_norm_just_corr.index))\n",
    "    list_correlation = np.full((num, num), 0.00000)\n",
    "    for j in range(len(var)-1):\n",
    "        for i in range(len(df_norm_just_corr.columns)-1):\n",
    "            list_correlation[j][i]=get_correlation_target(df_norm_just_corr,i,df_norm_just_corr.iloc[:,j])\n",
    "    df_correlations = pd.DataFrame(list_correlation)\n",
    "    df_correlations[df_correlations==1]=-1\n",
    "    list_max_correlations = df_correlations.max(axis=1).tolist()\n",
    "    #test = df_correlations.iloc[:,i].values\n",
    "    list_index_max = []\n",
    "    for i, max_i in enumerate(list_max_correlations):\n",
    "        test = list(df_correlations.iloc[:,i].values)\n",
    "        list_index_max.append(test.index(max_i))\n",
    "    list_index_max_couples = np.full((num, 2),0)\n",
    "    for i in range(0,len(var)):\n",
    "        list_index_max_couples[i][0] = i\n",
    "    for i in range(0,len(var)):\n",
    "        list_index_max_couples[i][1] = list_index_max[i]\n",
    "    list_to_delete = []\n",
    "    for i in range(1,len(df_corr)-1):\n",
    "        for j in range(i+1,len(df_corr.columns)):\n",
    "            if(list_index_max_couples[i][0] == list_index_max_couples[j][1]):\n",
    "                list_to_delete.append(j)\n",
    "    list_index_max_couples = np.delete(list_index_max_couples,list_to_delete,axis=0)\n",
    "    list_index_to_keep = []\n",
    "    for i in range(0,len(list_index_max_couples)):\n",
    "        if(df_corr.iloc[list_index_max_couples[i][0],0]>df_corr.iloc[list_index_max_couples[i][1],0]):\n",
    "            list_index_to_keep.append(list_index_max_couples[i][0])\n",
    "        else :\n",
    "            list_index_to_keep.append(list_index_max_couples[i][1])\n",
    "    df_corr_corr = df_corr.iloc[list_index_to_keep]\n",
    "    df_norm_just_corr = df_norm_just_corr.iloc[:,list_index_to_keep]\n",
    "    bb = sorted(list_index_to_keep)\n",
    "    list_index_corr_to_keep = [list_index_corr[val]for val in bb]\n",
    "    list_text_corr_corr = list_text_corr[list_index_to_keep]\n",
    "    df_norm_just_corr = pd.DataFrame(df_norm_just_corr,columns = list_index_to_keep)\n",
    "    df_norm_just_corr.columns = list_text_corr_corr\n",
    "    df_norm_just_corr = df_norm_just_corr.astype(float)\n",
    "    df_norm_just_corr = df_norm_just_corr.transpose().drop_duplicates().transpose()\n",
    "    list_index_corr_to_keep = list_without_duplicates(list_index_corr_to_keep)\n",
    "    return df_norm_just_corr, list_index_corr_to_keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the number of features by 2 twice \n",
    "def twice_corr(df_norm,df_corr,list_text,list_index_corr):\n",
    "    a,b = remove_too_much_correlations(df_norm,df_corr,list_text,list_index_corr,len(df_corr),df_corr)\n",
    "    c,d = remove_too_much_correlations(df_norm,a,list_text,b,len(a.columns),a.columns)\n",
    "    return c,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Divide the number of features by 2 once\n",
    "def twice_corr_once(df_norm,df_corr,list_text,list_index_corr):\n",
    "    a,b = remove_too_much_correlations(df_norm,df_corr,list_text,list_index_corr,len(df_corr),df_corr)\n",
    "    return a,b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results of EDA in a p file. X is for data set and y for target set \n",
    "def save_results_of_EDA(df,df_Energie,name):\n",
    "    pickle.dump(df , open( \"X_\"+name+\".p\", \"wb\" ) )\n",
    "    pickle.dump(df_Energie , open( \"y_\"+name+\".p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization of data for all the steps (15 min, 1 hour, 6 hours, 1 day and 1 week)\n",
    "def get_normalized_df_with_different_steps(df,bool_validation):\n",
    "    df_norm = normalization(df,bool_validation)\n",
    "    df_norm = df_norm.iloc[:,:-1]\n",
    "    df_norm.Energie = df.iloc[:,-1]\n",
    "    pickle.dump(df_norm , open( \"data_norm.p\", \"wb\" ) )\n",
    "    pickle.dump(df_norm.Energie , open( \"target.p\", \"wb\" ) )\n",
    "    df_norm_hour = df_changed_time_step(df.iloc[:,:-1],4,0)\n",
    "    df_norm_6_hour = df_changed_time_step(df.iloc[:,:-1],24,0) \n",
    "    df_norm_day = df_changed_time_step(df.iloc[:,:-1],96,0)\n",
    "\n",
    "    df_norm_week = df_changed_time_step(df.iloc[96*5:,:-1],96*7,0)\n",
    "\n",
    "\n",
    "    df_norm_hour.Energie = add_Energie_median(df,4)\n",
    "    df_norm_6_hour.Energie = add_Energie_median(df,24)\n",
    "    df_norm_day.Energie = add_Energie_median(df,96)\n",
    "    df_norm_week.Energie = add_Energie_median(df.iloc[96*5:],96*7)\n",
    "    return df_norm, df_norm_hour,df_norm_6_hour,df_norm_day,df_norm_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function launch statistics for all the type of time steps\n",
    "def get_data_from_statistics(df_norm, df_norm_hour,df_norm_6_hour,df_norm_day,df_norm_week):\n",
    "    df_corr_15_min, list_index_corr_15_min = launch_correlations(df_norm,df_norm.Energie,\"15_min\",0.7)\n",
    "    df_corr_hour, list_index_corr_hour = launch_correlations(df_norm_hour,df_norm_hour.Energie,\"hour\",0.7)\n",
    "    df_corr_6_hour, list_index_corr_6_hour = launch_correlations(df_norm_6_hour,df_norm_6_hour.Energie,\"6_hour\",0.7)\n",
    "    df_corr_day, list_index_corr_day = launch_correlations(df_norm_day,df_norm_day.Energie,\"day\",0.7)\n",
    "    df_corr_week, list_index_corr_week = launch_correlations(df_norm_week,df_norm_week.Energie,\"week\",0.8)\n",
    "\n",
    "    df_15_min, index_15_min = twice_corr(df_norm,df_corr_15_min,list_adress,list_index_corr_15_min)\n",
    "    df_hour,index_hour = twice_corr(df_norm_hour,df_corr_hour,list_adress,list_index_corr_hour)\n",
    "    df_6_hour,index_6_hour = twice_corr(df_norm_6_hour,df_corr_6_hour,list_adress,list_index_corr_6_hour)\n",
    "    df_day,index_day = twice_corr(df_norm_day,df_corr_day,list_adress,list_index_corr_day)\n",
    "    df_week,index_week = twice_corr_once(df_norm_week,df_corr_week,list_adress,list_index_corr_week)\n",
    "\n",
    "    df_corr_week2 = build_correlation_matrix_with_correlation_level(df_week,df_norm_week.Energie,0.7)\n",
    "\n",
    "    df_week2,index_week2 = twice_corr_once(df_norm_week,df_corr_week2,list_adress,index_week)\n",
    "    \n",
    "    return df_15_min, df_hour,df_6_hour,df_day,df_week2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Launch normalizaiton, statistics and save results in \".p file\"\n",
    "def launch_EDA_learning(df,name,bool_validation):\n",
    "    df_norm, df_norm_hour,df_norm_6_hour,df_norm_day,df_norm_week = get_normalized_df_with_different_steps(df,bool_validation)\n",
    "    df_15_min, df_hour,df_6_hour,df_day,df_week =  get_data_from_statistics(df_norm, df_norm_hour,df_norm_6_hour,df_norm_day,df_norm_week)\n",
    "    save_results_of_EDA(df_15_min,df_norm.Energie,name+\"15_min\")\n",
    "    save_results_of_EDA(df_hour,df_norm_hour.Energie,name+\"hour\")\n",
    "    save_results_of_EDA(df_6_hour,df_norm_6_hour.Energie,name+\"6_hour\")\n",
    "    save_results_of_EDA(df_day,df_norm_day.Energie,name+\"day\")\n",
    "    save_results_of_EDA(df_week,df_norm_week.Energie,name+\"week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find columns with any na values\n",
    "def get_index_columns_na_values(df_validation):\n",
    "    nan_columns = []\n",
    "    for i in range(len(df_validation.columns)):\n",
    "        if(df_validation[i].hasnans):\n",
    "            nan_columns.append(i)\n",
    "    return nan_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fulfill na values in validation sets\n",
    "def remove_na_bug_training_validation_sets(df_validation):\n",
    "    index_columns_na = get_index_columns_na_values(df_validation)\n",
    "    list_median = df.iloc[:,index_columns_na].median()\n",
    "    for i in range(len(index_columns_na)):\n",
    "        df_validation.iloc[:,index_columns_na[i]] = list_median[index_columns_na[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find index of column from previous EDA on learning set for validation set \n",
    "def get_index_column_EDA(columns):\n",
    "    list_index = []\n",
    "    for i in range(len(columns)):\n",
    "        list_index.append(list_adress.index(columns[i]))\n",
    "    return list_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler is the function to normalize data\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load of data from Pipeline Data Preparation \n",
    "df = pickle.load(open(\"data_total_prepared.p\", \"rb\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Last preparation before EDA\n",
    "df,list_adress,list_columns = prepare_df_to_get_correlations(df,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "launch_EDA_learning(df,'learning',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = pickle.load(open(\"data_validation_total_prepared.p\", \"rb\") )\n",
    "#df_validation = prepare_df_to_get_correlations(df_validation,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = prepare_df_to_get_correlations(df_validation,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch_EDA_validation(df_validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA for validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Because of a bug we keep data until 537th row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation = df_validation.iloc[:537,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation.Energie = df_validation.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_norm = normalization(df_validation.iloc[:,:-1],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_norm.Energie = df_validation.Energie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add of different time steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_validation_norm_hour = df_changed_time_step(df_validation.iloc[:,:-1],4,1)\n",
    "df_validation_norm_6_hour = df_changed_time_step(df_validation.iloc[:,:-1],24,1) \n",
    "df_validation_norm_day = df_changed_time_step(df_validation.iloc[:,:-1],96,1)\n",
    "df_validation_norm_week = df_changed_time_step(df_validation.iloc[96*4:,:-1],96*7,1)\n",
    "\n",
    "\n",
    "df_validation_norm_hour.Energie = add_Energie_median(df_validation,4)\n",
    "df_validation_norm_6_hour.Energie = add_Energie_median(df_validation,24)\n",
    "df_validation_norm_day.Energie = add_Energie_median(df_validation,96)\n",
    "df_validation_norm_week.Energie = add_Energie_median(df_validation.iloc[96*4:],96*7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get dat of EDA from learning set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_15min = pickle.load(open( \"X_learning15_min.p\", \"rb\"))\n",
    "X_hour = pickle.load(open( \"X_learninghour.p\", \"rb\"))\n",
    "X_6_hour = pickle.load(open( \"X_learning6_hour.p\", \"rb\"))\n",
    "X_day = pickle.load(open( \"X_learningday.p\", \"rb\"))\n",
    "X_week = pickle.load(open( \"X_learningweek.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_EDA_15min = list(X_15min.columns)\n",
    "columns_EDA_hour = list(X_hour.columns)\n",
    "columns_EDA_6_hour = list(X_6_hour.columns)\n",
    "columns_EDA_day = list(X_day.columns)\n",
    "columns_EDA_week = list(X_week.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_adress = list_adress.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We get the names of columns of EDA from learning set to put them on validation set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_column_EDA_15_min = get_index_column_EDA(columns_EDA_15min)\n",
    "index_column_EDA_hour = get_index_column_EDA(columns_EDA_hour)\n",
    "index_column_EDA_6_hour = get_index_column_EDA(columns_EDA_6_hour)\n",
    "index_column_EDA_day = get_index_column_EDA(columns_EDA_day)\n",
    "index_column_EDA_week = get_index_column_EDA(columns_EDA_week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_validation_15min = df_validation_norm.iloc[:,index_column_EDA_15_min]\n",
    "X_validation_hour = df_validation_norm_hour.iloc[:,index_column_EDA_hour]\n",
    "X_validation_6_hour = df_validation_norm_6_hour.iloc[:,index_column_EDA_6_hour]\n",
    "X_validation_day = df_validation_norm_day.iloc[:,index_column_EDA_day]\n",
    "X_validation_week = df_validation_norm_week.iloc[:,index_column_EDA_week]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_of_EDA(X_validation_15min,df_validation_norm.Energie,\"validation_15min\")\n",
    "save_results_of_EDA(X_validation_hour,df_validation_norm_hour.Energie,\"validation_hour\")\n",
    "save_results_of_EDA(X_validation_6_hour,df_validation_norm_6_hour.Energie,\"validation_6hour\")\n",
    "save_results_of_EDA(X_validation_day,df_validation_norm_day.Energie,\"validation_day\")\n",
    "save_results_of_EDA(X_validation_week,df_validation_norm_week.Energie,\"validation_week\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#launch_EDA(df_validation,'validation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
